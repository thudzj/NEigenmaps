MLP:
  All runs:
  Highest Train: 86.73 ± 0.24
  Highest Valid: 75.51 ± 0.09
  Final Train: 84.31 ± 0.93
   Final Test: 61.09 ± 0.13

minibatch based MLP decap=0.5:
   Run 02:
   Highest Train: 87.96
   Highest Valid: 77.36
     Final Train: 87.55
      Final Test: 62.52

python main_products.py --device 1 --runs 1 --output_dir ./logs/mlp-test2 --model mlp --blr 0.1 --alpha 0.3 --hidden_channels 1024 --use_bn
    Highest Train: 71.91 ± nan
    Highest Valid: 69.39 ± nan
    Final Train: 71.83 ± nan
    Final Test: 56.10 ± nan

python main_products.py --device 4 --runs 1 --output_dir ./logs/mlp-test2 --model mlp --blr 0.1 --alpha 0.3 --hidden_channels 1024 --use_bn
  Highest Train: 77.21 ± nan
  Highest Valid: 73.48 ± nan
  Final Train: 77.21 ± nan
  Final Test: 59.56 ± nan


----------new -----
python main_products.py --device 3 --runs 1 --model res_mlp --dataset arxiv --hidden_channels 1024 --num_layers 12 --proj_dim 8192 8192 --alpha 0.04 --lr 0.2 --weight_decay 0 --use_K_sqr --output_dir logs/res_mlp12l1024h_Ksqr_alpha0.04
  Run 01:
  Highest Train: 75.15
  Highest Valid: 69.44
    Final Train: 74.70
     Final Test: 68.06
    python main_products.py --device 3 --runs 1 --model res_mlp --dataset arxiv --hidden_channels 1024 --num_layers 12 --proj_dim 8192 8192 --alpha 0.04 --lr 0.2 --weight_decay 0 --use_K_sqr --output_dir logs/res_mlp12l1024h_Ksqr_alpha0.04 --ft_only --ft_mode freeze --lr-classifier 0.1 --lr-backbone 0.001 --ft_weight_decay 1e-6

    python main_products.py --device 7 --runs 1 --model res_mlp --dataset arxiv --hidden_channels 1024 --num_layers 12 --proj_dim 8192 8192 --alpha 0.04 --lr 0.2 --weight_decay 0 --use_K_sqr --output_dir logs/res_mlp12l1024h_Ksqr_alpha0.04 --ft_only --ft_mode finetune --ft_lr 0.01 --ft_weight_decay 1e-6 --ft_smoothing 0.1 --ft_mixup 0.8 --ft_layer_decay 0.01


python main_products.py --device 6 --runs 1 --model res_mlp --dataset arxiv --hidden_channels 2048 --num_layers 12 --proj_dim 8192 8192 --alpha 0.04 --lr 0.2 --weight_decay 0 --use_K_sqr --output_dir logs/res_mlp12l2048h_Ksqr_alpha0.04
    Run 01:
    Highest Train: 80.78
    Highest Valid: 69.37
    Final Train: 77.31
    Final Test: 68.01

python main_products.py --device 0 --runs 1 --model res_mlp --dataset arxiv --hidden_channels 256 --num_layers 12 --proj_dim 8192 8192 --alpha 0.04 --lr 0.2 --weight_decay 0 --joint --alpha_cls 0.01


---------------


python main_products.py --device 3 --runs 1 --model res_mlp --hidden_channels 1024 --num_layers 12 --proj_dim 8192 8192 --alpha 0.3 --lr 0.3 --weight_decay 0 --batch_size 16384 --K_normalize --epochs 20 --output_dir logs/products_al.3_lr.3
  Run 01:
  Highest Train: 86.08
  Highest Valid: 83.67
  Final Train: 86.08
  Final Test: 72.74

  python main_products.py --device 6 --runs 1 --model res_mlp --hidden_channels 1024 --num_layers 12 --proj_dim 8192 8192 --alpha 0.3 --lr 0.3 --weight_decay 0 --batch_size 16384 --K_normalize --epochs 20 --output_dir logs/products_al.3_lr.3 --ft_only --ft_mode freeze --ft_lr 0.01 --ft_weight_decay 1e-3

  python main_products.py --device 7 --runs 1 --model res_mlp --hidden_channels 1024 --num_layers 12 --proj_dim 8192 8192 --alpha 0.3 --lr 0.3 --weight_decay 0 --batch_size 16384 --K_normalize --epochs 20 --output_dir logs/products_al.3_lr.3 --ft_only --ft_mode finetune --ft_lr 0.01 --ft_weight_decay 1e-3 --ft_smoothing 0.1 --ft_layer_decay 0.01



python main_products.py --device 4 --model res_mlp --hidden_channels 2048 --num_layers 12 --proj_dim 8192 8192 --alpha 0.3 --lr 0.3 --weight_decay 0 --batch_size 16384 --K_normalize --epochs 20 --output_dir logs/products_al.3_lr.3_w2048
    Run 01:
    Highest Train: 89.91
    Highest Valid: 86.51
    Final Train: 89.91
     Final Test: 74.01

    python main_products.py --device 2 --model res_mlp --hidden_channels 2048 --num_layers 12 --proj_dim 8192 8192 --alpha 0.3 --lr 0.3 --weight_decay 0 --batch_size 16384 --K_normalize --epochs 20 --output_dir logs/products_al.3_lr.3_w2048 --ft_only --ft_mode freeze --ft_lr 0.01 --ft_weight_decay 1e-3
        **Post-finetuning results** Run: 01, loss: 0.3640, Train: 89.30%, Valid: 87.18%, Test: 75.54%

    python main_products.py --device 4 --model res_mlp --hidden_channels 2048 --num_layers 12 --proj_dim 8192 8192 --alpha 0.3 --lr 0.3 --weight_decay 0 --batch_size 16384 --K_normalize --epochs 20 --output_dir logs/products_al.3_lr.3_w2048 --ft_only --ft_mode finetune --ft_lr 0.01 --ft_weight_decay 1e-3 --ft_smoothing 0.1 --ft_layer_decay 0.01
        Run: 01, fine-tuning, Epoch: 43, Loss: 1.0014, Train: 92.14%, Valid: 87.01%, Test: 76.89%

        python main_products.py --device 0 --model res_mlp --hidden_channels 2048 --num_layers 12 --proj_dim 8192 8192 --alpha 0.3 --lr 0.3 --weight_decay 0 --batch_size 16384 --K_normalize --epochs 20 --output_dir logs/products_al.3_lr.3_w2048 --ft_only --ft_mode finetune --ft_lr 0.01 --ft_weight_decay 1e-3 --ft_smoothing 0.1 --ft_layer_decay 0.01 --runs 10
            All runs:
            Highest Train: 92.79 ± 0.48
            Highest Valid: 87.13 ± 0.10
            Final Train: 92.79 ± 0.48
             Final Test: 76.74 ± 0.14
            All runs:
            Highest Train: 97.41 ± 0.05
            Highest Valid: 92.15 ± 0.04
            Final Train: 97.41 ± 0.05
             Final Test: 84.26 ± 0.08


python main_products.py --device 3 --model res_mlp --hidden_channels 2048 --num_layers 12 --proj_dim 8192 8192 --alpha 0.3 --lr 0.3 --weight_decay 0 --batch_size 16384 --epochs 20 --output_dir logs/products_al.3_lr.3_w2048_new
    Test -- Epoch: 19, Train: 89.90%, Valid: 86.97%, Test: 74.88%


    python main_products.py --device 2 --model res_mlp --hidden_channels 2048 --num_layers 12 --proj_dim 8192 8192 --alpha 0.3 --lr 0.3 --weight_decay 0 --batch_size 16384 --epochs 20 --output_dir logs/products_al.3_lr.3_w2048_new  --ft_only --ft_mode freeze --ft_lr 0.01 --ft_weight_decay 1e-3 --ft_smoothing 0.1 --runs 10; python main_products.py --device 2 --model res_mlp --hidden_channels 2048 --num_layers 12 --proj_dim 8192 8192 --alpha 0.3 --lr 0.3 --weight_decay 0 --batch_size 16384 --epochs 20 --output_dir logs/products_al.3_lr.3_w2048_new  --ft_only --ft_mode freeze --ft_lr 0.01 --ft_weight_decay 1e-3 --ft_smoothing 0.1 --runs 10 --ft_label_ratio 0.1; python main_products.py --device 2 --model res_mlp --hidden_channels 2048 --num_layers 12 --proj_dim 8192 8192 --alpha 0.3 --lr 0.3 --weight_decay 0 --batch_size 16384 --epochs 20 --output_dir logs/products_al.3_lr.3_w2048_new  --ft_only --ft_mode freeze --ft_lr 0.01 --ft_weight_decay 1e-3 --ft_smoothing 0.1 --runs 10 --ft_label_ratio 0.01
      All runs:
      Highest Train: 88.84 ± 0.02
      Highest Valid: 87.56 ± 0.02
        Final Train: 88.84 ± 0.02
         Final Test: 76.93 ± 0.04
      All runs:
       Highest Train: 87.25 ± 0.22
       Highest Valid: 86.29 ± 0.06
         Final Train: 87.25 ± 0.22
          Final Test: 74.48 ± 0.39
      All runs:
      Highest Train: 82.62 ± 0.42
      Highest Valid: 81.62 ± 0.49
        Final Train: 82.62 ± 0.42
         Final Test: 67.84 ± 0.79


    # do not use these finetuning results
    python main_products.py --device 3 --model res_mlp --hidden_channels 2048 --num_layers 12 --proj_dim 8192 8192 --alpha 0.3 --lr 0.3 --weight_decay 0 --batch_size 16384 --epochs 20 --output_dir logs/products_al.3_lr.3_w2048_new  --ft_only --ft_mode finetune --ft_lr 0.01 --ft_weight_decay 1e-3 --ft_smoothing 0.1 --ft_layer_decay 0.01 --runs 10; python main_products.py --device 3 --model res_mlp --hidden_channels 2048 --num_layers 12 --proj_dim 8192 8192 --alpha 0.3 --lr 0.3 --weight_decay 0 --batch_size 16384 --epochs 20 --output_dir logs/products_al.3_lr.3_w2048_new  --ft_only --ft_mode finetune --ft_lr 0.01 --ft_weight_decay 1e-3 --ft_smoothing 0.1 --ft_layer_decay 0.01 --runs 10 --ft_label_ratio 0.1; python main_products.py --device 3 --model res_mlp --hidden_channels 2048 --num_layers 12 --proj_dim 8192 8192 --alpha 0.3 --lr 0.3 --weight_decay 0 --batch_size 16384 --epochs 20 --output_dir logs/products_al.3_lr.3_w2048_new  --ft_only --ft_mode finetune --ft_lr 0.01 --ft_weight_decay 1e-3 --ft_smoothing 0.1 --ft_layer_decay 0.01 --runs 10 --ft_label_ratio 0.01
    All runs:
      Highest Train: 93.60 ± 0.27
      Highest Valid: 87.76 ± 0.08
        Final Train: 93.60 ± 0.27
         Final Test: 77.63 ± 0.10
    All runs:
      Highest Train: 86.77 ± 0.34
      Highest Valid: 85.30 ± 0.11
        Final Train: 86.77 ± 0.34
         Final Test: 74.44 ± 0.59
    All runs:
       Highest Train: 80.55 ± 0.55
       Highest Valid: 79.83 ± 0.62
         Final Train: 80.55 ± 0.55
          Final Test: 66.88 ± 0.68


    python main_products.py --device 4 --model res_mlp --hidden_channels 2048 --num_layers 12 --proj_dim 8192 8192 --alpha 0.3 --lr 0.3 --weight_decay 0 --batch_size 16384 --epochs 20 --output_dir logs/products_al.3_lr.3_w2048_new  --ft_only --ft_mode finetune --ft_lr 0.01 --ft_weight_decay 1e-3 --ft_smoothing 0.1 --ft_layer_decay 1 --runs 10 --ft_from_sractch; python main_products.py --device 5 --model res_mlp --hidden_channels 2048 --num_layers 12 --proj_dim 8192 8192 --alpha 0.3 --lr 0.3 --weight_decay 0 --batch_size 16384 --epochs 20 --output_dir logs/products_al.3_lr.3_w2048_new  --ft_only --ft_mode finetune --ft_lr 0.01 --ft_weight_decay 1e-3 --ft_smoothing 0.1 --ft_layer_decay 1 --runs 10 --ft_label_ratio 0.1 --ft_from_sractch; python main_products.py --device 6 --model res_mlp --hidden_channels 2048 --num_layers 12 --proj_dim 8192 8192 --alpha 0.3 --lr 0.3 --weight_decay 0 --batch_size 16384 --epochs 20 --output_dir logs/products_al.3_lr.3_w2048_new  --ft_only --ft_mode finetune --ft_lr 0.01 --ft_weight_decay 1e-3 --ft_smoothing 0.1 --ft_layer_decay 1 --runs 10 --ft_label_ratio 0.01 --ft_from_sractch
      All runs:
      Highest Train: 99.22 ± 0.03
      Highest Valid: 81.12 ± 0.11
        Final Train: 99.22 ± 0.03
         Final Test: 65.34 ± 0.06

     All runs:
     Highest Train: 77.77 ± 0.14
     Highest Valid: 73.43 ± 0.28
       Final Train: 77.77 ± 0.14
        Final Test: 58.90 ± 0.22

        All runs:
      Highest Train: 61.24 ± 0.39
      Highest Valid: 60.17 ± 0.36
        Final Train: 61.24 ± 0.39
         Final Test: 48.16 ± 0.40


python main_products.py --device 3 --model res_mlp --hidden_channels 2048 --num_layers 12 --proj_dim 8192 8192 --alpha 0.3 --lr 0.3 --weight_decay 0 --batch_size 16384 --epochs 20 --no_stop_grad --output_dir logs/products_al.3_lr.3_w2048_nosg

      python main_products.py --device 3 --model res_mlp --hidden_channels 2048 --num_layers 12 --proj_dim 8192 8192 --alpha 0.3 --lr 0.3 --weight_decay 0 --batch_size 16384 --epochs 20 --no_stop_grad --output_dir logs/products_al.3_lr.3_w2048_nosg  --ft_only --ft_mode freeze --ft_lr 0.01 --ft_weight_decay 1e-3 --ft_smoothing 0.1 --runs 10
      All runs:
    Highest Train: 89.99 _ 0.05
    Highest Valid: 88.14 _ 0.02
      Final Train: 89.99 _ 0.05
       Final Test: 78.33 _ 0.08

      python main_products.py --device 4 --model res_mlp --hidden_channels 2048 --num_layers 12 --proj_dim 8192 8192 --alpha 0.3 --lr 0.3 --weight_decay 0 --batch_size 16384 --epochs 20 --no_stop_grad --output_dir logs/products_al.3_lr.3_w2048_nosg  --ft_only --ft_mode freeze --ft_lr 0.01 --ft_weight_decay 1e-3 --ft_smoothing 0.1 --runs 10 --ft_label_ratio 0.1
      All runs:
      Highest Train: 88.05 _ 0.24
      Highest Valid: 86.72 _ 0.15
        Final Train: 88.05 _ 0.24
         Final Test: 75.78 _ 0.46

      python main_products.py --device 5 --model res_mlp --hidden_channels 2048 --num_layers 12 --proj_dim 8192 8192 --alpha 0.3 --lr 0.3 --weight_decay 0 --batch_size 16384 --epochs 20 --no_stop_grad --output_dir logs/products_al.3_lr.3_w2048_nosg  --ft_only --ft_mode freeze --ft_lr 0.01 --ft_weight_decay 1e-3 --ft_smoothing 0.1 --runs 10 --ft_label_ratio 0.01
      All runs:
    Highest Train: 83.08 _ 0.19
    Highest Valid: 82.21 _ 0.30
      Final Train: 83.08 _ 0.19
       Final Test: 68.04 _ 0.39

CUDA_VISIBLE_DEVICES=2 python gnn.py   --label_ratio 1/0.1/0.01
    All runs:
    Highest Train: 93.54 _ 0.09
  Highest Valid: 91.99 _ 0.07
    Final Train: 93.54 _ 0.09
     Final Test: 75.72 _ 0.31

    All runs:
    Highest Train: 90.87 _ 0.14
Highest Valid: 89.63 _ 0.10
  Final Train: 90.87 _ 0.14
   Final Test: 73.14 _ 0.34

     All runs:
     Highest Train: 85.62 _ 0.26
Highest Valid: 84.70 _ 0.29
  Final Train: 85.62 _ 0.26
   Final Test: 67.61 _ 0.48


CUDA_VISIBLE_DEVICES=4 python node2vec.py; CUDA_VISIBLE_DEVICES=4 python mlp.py --use_node_embedding --hidden_channels 512 --label_ratio 1/0.1/0.01
All runs:
Highest Train: 94.87 _ 0.80
Highest Valid: 90.25 _ 0.11
Final Train: 94.87 _ 0.80
 Final Test: 72.50 _ 0.46
 All runs:
Highest Train: 88.66 _ 0.17
Highest Valid: 86.91 _ 0.08
 Final Train: 88.66 _ 0.17
  Final Test: 68.72 _ 0.43
  All runs:
Highest Train: 81.61 _ 0.41
Highest Valid: 80.31 _ 0.40
  Final Train: 81.61 _ 0.41
   Final Test: 61.97 _ 0.44

CUDA_VISIBLE_DEVICES=4 python mlp.py --hidden_channels 512 --label_ratio 1/0.1/0.01
  All runs:
  Highest Train: 86.78 _ 1.36
  Highest Valid: 76.85 _ 0.14
  Final Train: 86.78 _ 1.36
   Final Test: 62.16 _ 0.15
   All runs:
  Highest Train: 73.36 _ 0.16
  Highest Valid: 70.63 _ 0.15
    Final Train: 73.36 _ 0.16
     Final Test: 57.44 _ 0.20
    All runs:
  Highest Train: 60.25 _ 0.45
  Highest Valid: 59.28 _ 0.49
    Final Train: 60.25 _ 0.45
     Final Test: 47.76 _ 0.62

python3 mlp_cs.py --device 4 --use_embed  --use_cached --label_ratio 1/0.1/0.01
  All runs:
  Highest Train: 87.27 _ 0.61
  Highest Valid: 82.90 _ 0.35
    Final Train: 87.27 _ 0.61
     Final Test: 64.21 _ 0.35

   All runs:
   Highest Train: 78.78 _ 0.18
   Highest Valid: 74.53 _ 0.27
     Final Train: 78.78 _ 0.18
      Final Test: 58.99 _ 0.20
      All runs:
    Highest Train: 64.00 _ 0.47
    Highest Valid: 62.54 _ 0.58
      Final Train: 64.00 _ 0.47
       Final Test: 49.94 _ 0.30


time comparon: on one rtx3090
gcn on all test data Avg execution time (s): 0.3818
our on all test data Avg execution time (s): 3.3939
our on one test data Avg execution time (s): 0.0013
